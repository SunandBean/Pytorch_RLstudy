# Ch5. 딥러닝을 적용한 강화학습 - DQN 구현

## 5.1 딥러닝을 적용한 Q러닝

### 표형식 표현의 문제점

**Q러닝** 

- 표형식 표현
  - 행 : 에이전트의 상태 / 열 : 에이전트가 취할 수 있는 행동
  - 셀의 각 값 : Q(s,a)
- 문제점
  - 상태 변수의 종류가 늘어나거나 각 변수를 세세하게 분할하는 경우 행의 수가 급격히 증가
  - 이를 학습시키기 위해서는 에피소드 수도 많이 필요
- 딥러닝으로 해결하려는 것
  - "상태변수 수가 많아지면 표형식 표현으로는 강화학습이 어렵다"

### 심층강화학습 알고리즘 DQN

행동가치 함수

- 표형식 -> (층 수가 많은) 신경망

역진자 태스크에서의 **신경망**

- 입력 : 상태변수의 값
  - 유닛 수 : 상태변수의 수
- 출력 : 행동
  - 유닛 수 : 행동의 가짓수
  - 출력값 : 행동가치 함수 Q(s,a)

결합 가중치를 학습시키는 방법

- = 역전파 계산에 어떤 오차함수를 사용해 오차를 계산해야 하는가!

- Q러닝 알고리즘

  - 행동가치 함수Q의 수정 식

  $$
  Q(s_t,a_t) = Q(s_t,a_t)+\eta *(R_{t+1} + \gamma max_a Q(s_{t+1},a)\ -\ Q(s_t,a_t))
  $$

  - 위와 같은 수정식을 사용하는 이유는 결국
    $$
    Q(s_t,a_t) = R_{t+1}+\gamma max_a Q(s_{t+1},a)
    $$

  - 위와 같은 수식을 만족시키기 위해서이다.

  - 따라서, 실제 출력과 원하는 값의 차이를 계산하여 그 제곱을 오차로 사용 (제곱오차함수)
    $$
    E(s_t,a_t) = (R_{t+1}+\gamma max_a Q(s_{t+1},a)-Q(s_t,a_t))^2
    $$

  - 상태 s(t+1)은 s(t)에서 a(t)를 취해보고 알아냄

  - max_a Q(s(t),a(t))의 값은 신경망에 상태 s(t+1)을 입력해서 알아냄

---

## 5.2 DQN을 구현할 때 중요한 4가지 기법

#### Experience Replay

메모리에 각 단계의 내용을 저장해 둔 다음 무작위로 내용을 꺼내 신경망을 학습

시간적 상관관계를 줄여줌

#### Fixed Target Q-Network

두 가지 신경망 사용

- 행동을 결정하는 main-network
- 오차함수 계산 시에 행동가치를 계산하는 target-network
  - max_a Q(s_{t+1},a) 계산 시 이용

#### 보상 클리핑

각 단계에서 받은 보상을 -1, 0, 1의 세 가지 값 중 하나로 고정하는 방법

=> 태스크의 보상 설계와 상관없이 같은 하이퍼 파라미터로 DQN을 적용할 수 있다는 장점

#### 제곱오차함수 대신 Huber 함수로 오차 계산

오차가 큰 경우에 제곱오차를 사용하면 오차함수의 출력이 지나치게 커져서 학습이 불안정해지기 쉬워짐

---

## 5.3 DQN 구현(1)

### 파이토치로 DQN을 구현할 때 주의점

주의해야 할 5가지 요소

1. Experience Replay 및 유사 Fixed Target Q-Network를 구현하기 위해 미니배치 학습을 적용
   - 다음 상태의 존재 여부에 따라 분기하도록 구현!
2. 파이토치에서 미니배치를 다루는 방법
   - 미니배치를 효율적으로 다루기 위한 구현을 위해서 연습 필요
3. 변수의 데이터 타입
   - 신경망이 서로 데이터를 주고 받아야 한다.
   - => NumPy 타입과 텐서 타입 간의 변환이 잦음
4. 변수의 크기
5. namedtuple을 사용한다는 점

### DQN 구현 

클래스 정의

- Replay Memory 
  - 경험 데이터를 저장하는 역할
  - 메서드
    - push : 각 단계에서 해당 단계의 transition을 저장
    - sample : 무작위로 선택된 transition을 batch size에 맞게 추출
- Brain
  - ★ DQN 구현의 핵심
  - 메서드
    - replay : 메모리 클래스에서 미니배치를 꺼내와서 신경망의 결합 가중치를 학습하는 방법으로 Q함수 수정
    - decide_action : e-greedy 알고리즘으로 무작위로 선택된 행동 혹은 현재 상태에서 Q값이 최대가 되는 행동을 선택해서 그 행동의 인덱스 값을 반환

---

## 5.4 DQN 구현(2)

클래스 정의

- Agent 
  - 수레 역할
  - 메서드
    - update_q_function
    - get_action
    - memorize : 메모리 객체에 transition을 저장할 때 사용
- Environment
  - 실행 환경
    - 차이점 
      - 관측 결과인 observation을 그대로 state로 사용! 
      - 최근 10에피소드에 걸린 단계 수를 저장하여 평균값을 계산, 학습 진행상황을 알기 쉽게 한다